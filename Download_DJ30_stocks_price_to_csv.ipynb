{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# Pretend use browser\n",
    "headers = {'User-Agent': 'Mozilla/75.0 Chrome/80.0.3987.163 '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  stock symbol\n",
    "def save_dj30_symbols():\n",
    "    \n",
    "    # crawler websites\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average',headers = headers)\n",
    "    \n",
    "    # source is text, parser is lxml\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    \n",
    "    # table\n",
    "    table = soup.find_all(\"table\",{\"id\" : \"constituents\"})[0]\n",
    "    \n",
    "    \n",
    "    symbols = []\n",
    "    \n",
    "    #tr is row of table\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        \n",
    "        # soup object get text)\n",
    "        symbol = row.findAll('td')[0].get_text(strip=True)\n",
    "\n",
    "        symbols.append(symbol)\n",
    "        \n",
    "    #  save result to pickle\n",
    "    with open(\"dj30.pickle\",\"wb\") as file:\n",
    "        pickle.dump(symbols, file)\n",
    "    \n",
    "    return symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3M',\n",
       " 'American Express',\n",
       " 'Apple Inc.',\n",
       " 'Boeing',\n",
       " 'Caterpillar Inc.',\n",
       " 'Chevron Corporation',\n",
       " 'Cisco Systems',\n",
       " 'The Coca-Cola Company',\n",
       " 'Dow Inc.',\n",
       " 'ExxonMobil',\n",
       " 'Goldman Sachs',\n",
       " 'The Home Depot',\n",
       " 'IBM',\n",
       " 'Intel',\n",
       " 'Johnson & Johnson',\n",
       " 'JPMorgan Chase',\n",
       " \"McDonald's\",\n",
       " 'Merck & Co.',\n",
       " 'Microsoft',\n",
       " 'Nike',\n",
       " 'Pfizer',\n",
       " 'Procter & Gamble',\n",
       " 'Raytheon Technologies',\n",
       " 'The Travelers Companies',\n",
       " 'UnitedHealth Group',\n",
       " 'Verizon',\n",
       " 'Visa Inc.',\n",
       " 'Walmart',\n",
       " 'Walgreens Boots Alliance',\n",
       " 'The Walt Disney Company']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dj30_symbols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save price from spider\n",
    "def get_data_from_datareader(beg_y, beg_m, beg_d, end_y, end_m, end_d, reload_dj30=False):\n",
    "\n",
    "    # if reload, execute save_dj30_symbols()\n",
    "    if reload_dj30:\n",
    "        symbols = save_dj30_symbols()\n",
    "    # if no reload, read dj30.pickle     \n",
    "    else:\n",
    "        with open(\"dj30.pickle\",\"rb\") as file:\n",
    "            symbols = pickle.load(file)\n",
    "    \n",
    "    # if no folder exist, mkdir\n",
    "    if not os.path.exists('dj30'):\n",
    "        os.makedirs('dj30')\n",
    "        \n",
    "    beg = dt.datetime(beg_y, beg_m, beg_d)\n",
    "    end = dt.datetime(end_y, end_m, end_d)\n",
    "#     print(beg,end)\n",
    "\n",
    "    #for loop price from 'stooq'\n",
    "    for symbol in symbols:\n",
    "        #try: check symbol is downloaed\n",
    "        try:\n",
    "            if not os.path.exists('dj30/{}.csv'.format(symbol)):\n",
    "                df = web.DataReader(symbol, 'stooq', beg, end)\n",
    "                df.to_csv('dj30/{}.csv'.format(symbol))\n",
    "            else:\n",
    "                print('already have {}'.format(symbol))\n",
    "        # except\n",
    "        except:\n",
    "            print(symbol + \"can not download\")\n",
    "    print(\"crawler done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawler done\n"
     ]
    }
   ],
   "source": [
    "get_data_from_datareader(2020,1,1,2020,4,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
